# Black Box Adversarial Prompting for Foundation Models

This repository contains the implementation and replication of experiments from the paper "Black Box Adversarial Prompting for Foundation Models." The focus of this project is to explore the effects of adversarial prompts on foundation models, specifically examining their responses in text-to-text and text-to-image generation tasks.

## Overview

The repository is structured into two main directories:

- **Text-to-Image**: Contains code, notebooks, and results for image generation tasks. This includes experiments across various threat models, demonstrating how subtle changes in prompts can lead to significant differences in the output images generated by AI models.

- **Text-toText**: Contains code, notebooks, and findings related to text generation experiments. It showcases the impact of adversarial prompting on language models and provides insights into optimizing prompts for specific outcomes.

## Experiments

The experiments were conducted using models that fit within the technical limitations of a standard personal computer, utilizing Google Colab's GPU resources for efficient computation. This adaptation ensures that the experiments are replicable by a wider audience, even those with limited access to high-end computational resources.

Each directory contains Jupyter notebooks with detailed code and comments explaining the steps taken in the experiments, as well as the results obtained, which include both the expected outcomes and adversarial deviations.


